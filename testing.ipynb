{"cells":[{"metadata":{"_uuid":"7051a43c-c9df-4420-9d9b-f96ef0a1c8f4","_cell_guid":"a6a41e15-edae-4c0c-8a4c-bc45c750c114","trusted":true},"cell_type":"code","source":"import os\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96bbe960-510f-41b2-9984-2741a104ce75","_cell_guid":"82c39051-75d8-40ae-8335-2d8eb5612823","trusted":true},"cell_type":"markdown","source":"# **DEFINING THE CONVOLUTIONAL NEURAL NETWORK**"},{"metadata":{"_uuid":"7455f19f-bdbc-4262-ad9c-edf55a1d4ed6","_cell_guid":"3e52746e-574c-44af-b62c-b9948a4cf773","trusted":true},"cell_type":"code","source":"class ConvNet(nn.Module):\n\n    def __init__(self):\n        super(ConvNet, self).__init__()        \n        \n        self.ac = nn.ReLU()\n        self.pool = nn.MaxPool2d(2)\n        \n        self.conv1 = nn.Conv2d(1,100,3)\n        self.conv2 = nn.Conv2d(100,200,3)\n        self.drop = nn.Dropout(0.2)\n        \n        self.fc1 = nn.Linear(200*23*23, 50)\n        self.fc2 = nn.Linear(50, 2)\n\n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.ac(x)\n        x = self.pool(x)\n        \n        x = self.conv2(x)\n        x = self.ac(x)\n        x = self.pool(x)\n        \n        x = x.view(-1,200*23*23)\n        \n        x = self.drop(x)\n        \n        x = self.fc1(x)\n        \n        x = self.fc2(x)\n        return x\n\nmodel = torch.load(\"../input/modellll/model.pth\")\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1354caa7-299b-4a71-8f0e-87f3e9b4a4f6","_cell_guid":"811f33e0-89f9-4fe4-a09e-da9dfc932ba3","trusted":true},"cell_type":"markdown","source":"# **LOADING THE SOURCE VIDEO AND DEFINING FACE CLASSIFIER**"},{"metadata":{"_uuid":"724adc5b-eda2-48dc-86ca-37b38c058a7f","_cell_guid":"8857394d-0377-4415-9a35-ed0d18033852","trusted":true},"cell_type":"code","source":"face_clsfr=cv.CascadeClassifier('../input/haaaaaaas/haas.xml')\n\nsource=cv.VideoCapture('../input/anishhhh/anish_with_no_mask.mp4')\n\nlabels_dict={0:'DOOR OPEN',1:'DOOR NOT OPEN'}\ncolor_dict={0:(0,255,0),1:(0,0,255)}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f182eb9a-5de9-4bbf-bc1b-c89ef5fa48f0","_cell_guid":"c3b49e20-f676-4968-bcea-9ecea38d547f","trusted":true},"cell_type":"markdown","source":"# **HUMAN DETECTION USING YOLO ALGORITHM**"},{"metadata":{"_uuid":"eb77b674-ee03-4370-b312-7e8cc8f986f2","_cell_guid":"28412f40-0892-465a-b47d-14c02e166aee","trusted":true},"cell_type":"code","source":"human=[]\n\nwhile True:\n    \n    ret,image=source.read()\n    if ret==False:\n        break\n    \n    classes = None\n    with open('../input/maskdetection/coco.names', 'r') as f:\n        classes = [line.strip() for line in f.readlines()]\n\n\n    Width = image.shape[1]\n    Height = image.shape[0]\n\n    net = cv.dnn.readNet('../input/maskdetection/yolov3.weights', '../input/maskdetection/yolov3.cfg')\n\n    net.setInput(cv.dnn.blobFromImage(image, 0.00392, (416,416), (0,0,0), True, crop=False))\n\n    layer_names = net.getLayerNames()\n    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n    outs = net.forward(output_layers)\n\n\n    class_ids = []\n    confidences = []\n    boxes = []\n\n    #create bounding box \n    for out in outs:\n        for detection in out:\n            scores = detection[5:]\n            class_id = np.argmax(scores)\n            confidence = scores[class_id]\n            if confidence > 0.1:\n                center_x = int(detection[0] * Width)\n                center_y = int(detection[1] * Height)\n                w = int(detection[2] * Width)\n                h = int(detection[3] * Height)\n                x = center_x - w / 2\n                y = center_y - h / 2\n                class_ids.append(class_id)\n                confidences.append(float(confidence))\n                boxes.append([x, y, w, h])\n\n    indices = cv.dnn.NMSBoxes(boxes, confidences, 0.1, 0.1)\n\n    for i in indices:\n        i = i[0]\n        box = boxes[i]\n        if class_ids[i]==0:\n            label = str(classes[class_id]) \n            cv.rectangle(image, (round(box[0]),round(box[1])), (round(box[0]+box[2]),round(box[1]+box[3])), (255, 0, 0), 8)\n            img=image[round(box[1]):round(box[1]+box[3]),round(box[0]):round(box[0]+box[2])]\n            plt.imshow(image)\n            human.append(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fccb373-5a4b-42ec-a989-c59eb1748eca","_cell_guid":"b5de4bef-ad38-49ac-98c5-961df602fc73","trusted":true},"cell_type":"markdown","source":"# **FACE DETECTION USING VIOLA JONES ALGORITHM**"},{"metadata":{"_uuid":"b6dcee39-c3d9-4062-82d7-31fa402ddab7","_cell_guid":"283ddbcb-f408-48fb-ae2c-e42cf66df1f4","trusted":true},"cell_type":"code","source":"face=[]\n\ncascPathface = os.path.dirname(\n    cv.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\ncascPatheyes = os.path.dirname(\n    cv.__file__) + \"/data/haarcascade_eye_tree_eyeglasses.xml\"\n\nfaceCascade = cv.CascadeClassifier(cascPathface)\neyeCascade = cv.CascadeClassifier(cascPatheyes)\n\nfor i in range(len(human)):\n    frame = human[i]\n    if(frame.shape[0]!=0 and frame.shape[1]!=0):\n        gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n        faces = faceCascade.detectMultiScale(gray,\n                                             scaleFactor=1.1,\n                                             minNeighbors=5,\n                                             minSize=(60, 60),\n                                             flags=cv.CASCADE_SCALE_IMAGE)\n        for (x,y,w,h) in faces:\n            cv.rectangle(frame, (x, y), (x + w, y + h),(0,255,0), 2)\n            faceROI = frame[y:y+h,x:x+w]\n            new_frame=frame[round(y):round(y+h),round(x):round(x+w)]\n            face.append(new_frame)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(face[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dddb72f-2f24-4cdf-a374-cbe6e113c133","_cell_guid":"09a5ea30-a2e4-42fb-9714-a4e50ed91c78","trusted":true},"cell_type":"markdown","source":"# **MASK DETECTION USING TRAINED MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"size=(human[0].shape[1],human[0].shape[0])\nout=cv.VideoWriter('anish_no_mask.mp4',cv.VideoWriter_fourcc(*'DIVX'), 15, size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70b8bd0b-6b44-49b7-846e-b2597ae1a7ee","_cell_guid":"e2bdb5b0-5827-45b0-8534-67153fceb9c7","trusted":true},"cell_type":"code","source":"from scipy.special import softmax\nfor i in range(len(human)):\n\n    img=human[i]\n    if(img.shape[0]!=0 and img.shape[1]!=0):\n        gray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n        faces=face_clsfr.detectMultiScale(gray,1.3,5)  \n        img=cv.resize(img,(human[0].shape[1],human[0].shape[0]))\n        \n        for x,y,w,h in faces:\n            face_img=gray[y:y+w,x:x+w]\n            resized=cv.resize(face_img,(100,100))\n            normalized=resized/255.0\n            reshaped=np.reshape(normalized,(1,100,100,1))\n\n            ten_reshaped = torch.from_numpy(reshaped.astype(np.float32))\n            ten_reshaped = ten_reshaped.view(-1,1,100,100)\n            result=model(ten_reshaped)\n\n            result = result.detach().numpy()\n            result = softmax(result)\n            label=np.argmax(result,axis=1)[0]\n            if(result[0][0]>=0.30):\n                label=0\n            else:\n                label=1\n            cv.rectangle(img,(x,y),(x+w,y+h),color_dict[label],2)\n            cv.rectangle(img,(x,y-40),(x+w,y),color_dict[label],-1)\n            cv.putText(img, labels_dict[label], (x, y-10),cv.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n            out.write(img)\n        \nsource.release()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}